# 20377199 赵芮箐 第2周作业

![image-20220912113905715](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220912113905715.png)

>注意：不要使用jieba等库中提供的函数实现特征词抽取和文档表示，要求自己使用相关数据结构来实现；要通过函数对代码进行封装，并在main函数中调用。

### 任务一：处理文档

###### 1.使用danmuku.csv，其中一个弹幕可以视为一个文档（document），读入文档并分词（可以使用jieba或pyltp）。

![image-20220913173452033](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220913173452033.png)

### 任务二：词频统计
###### 2.过滤停用词（可用stopwords_list.txt，或自己进一步扩充）并统计词频，输出特定数目的高频词和低频词进行观察。建议将停用词提前加入到jieba等分词工具的自定义词典中，避免停用词未被正确分词。

![image-20220916100838056](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220916100838056.png)

**统计的前30个高频词和后30个低频词如下：**

![image-20220914152521128](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914152521128.png)

**结果分析：**

- 高频词

  包括B站常见弹幕，如：哈哈哈哈、啊啊啊、考古、笑死、真爱等；

  还包括和B站up相关的弹幕，如：好吃、独头蒜、树梢、大蒜等；（看出来up是**盗月社**啦）

  还包括和B站视频相关的弹幕，如：武汉、加油、藕、粉、牛杂等；（应该是去武汉的视频，还是武汉加油时期前一点的，找到了一条应该在里面的视频：【你吃过自助街吗！整条街随便点，随便坐，随便结账，就是开心！【深夜加餐饭04】】 https://www.bilibili.com/video/BV1mt411F78T?share_source=copy_web&vd_source=8674a9c5608547ccd52d1f4a57ec2e72）

  当然还是有些常见及情绪表达词汇，如：真的、热情、可爱、喜欢等。

- 低频词

  一些比较专有的、不常用、个人表达的词汇。

###### 3.根据词频进行特征词筛选，如只保留高频词，删除低频词（出现次数少于5之类），并得到特征词组成的特征集。

![image-20220913173554899](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220913173554899.png)

**过滤了词频低于5000的词，获得特征词集如下：**

![image-20220914153459175](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914153459175.png)

###### 6.（附加）能不能对高频词（如top 50之类）进行可视化呈现（WordCloud包）？

![image-20220913173617629](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220913173617629.png)

**用特征词集做出词云图如下：**

![](D:\code\mp2022\week1\word_cloud.png)

###### 7.（附加）能不能考虑别的特征词构建思路，如常用的TF-IDF，即一方面词的频率要高，另一方面，词出现的文档数越少越好，观察其与仅利用词频所得的结果有何差异，哪个更好？ 

![image-20220916100922149](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220916100922149.png)

![image-20220915003007787](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220915003007787.png)

**得到特征词集如下：**

![image-20220915091402506](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220915091402506.png)

**结果分析：**

与仅统计词频得到的特征词集进行对比分析：

![image-20220914153459175](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914153459175.png)

发现词语其实大差不差，顺序会有些区别。

像比如”真“这个词排序会下降，因为”真“这个词太普遍了，甚至可能会在一条弹幕里多次出现，因此词频很高，但其所在的文档数也会很多，故TF-IDF值会较小，顺序会下降。而像“考古”这种，会比较特别，所在的文档数不会那么多，故TF-IDF值会更大，顺序上升。

所以我觉得TF-IDF会更好一点，考虑该词出现的文档数越少越好，选出的特征词不会那么普遍，会更具有代表性。并且计算得到的TF-IDF值介于0-1之间，会更量化词的重要程度，也可以基于此来生成语料库的向量空间。

### 任务三：弹幕分析

###### 4.利用特征集为每一条弹幕生成向量表示，可以是0，1表示（one-hot，即该特征词在弹幕中是否出现）也可以是出现次数的表示（该特征词在弹幕中出现了多少次）。注意，可能出现一些过短的弹幕，建议直接过滤掉。

![image-20220914233953600](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914233953600.png)

###### 5.利用该向量表示，随机找几条弹幕，计算不同弹幕间的语义相似度，可尝试多种方式，如欧几里得距离或者余弦相似度等，并观察距离小的样本对和距离大的样本对是否在语义上确实存在明显的差别。请思考，这种方法有无可能帮助我们找到最有代表性的弹幕？

- **语义相似度计算：**

![image-20220914131501111](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914131501111.png)

>PS：这里限制了随机找到的弹幕是含有特征词的，不然找到的弹幕不含特征词，即为0向量，会认为其他的不含特征词的向量与之最近，但其实语义会各不相同。这也是这个方法的一个问题。

**运行结果举例：**

![image-20220914205515715](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914205515715.png)

![image-20220914181749448](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914181749448.png)

![image-20220914181825183](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914181825183.png)

![image-20220914213215633](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914213215633.png)

**结果分析：**

通过计算欧几里得距离大小，找到语义最相近和相远的弹幕。

语义最近的弹幕组效果还是挺明显的，含有至少一个相同的特征词，语义也基本相似。

（发现有重复弹幕后，在获得one-hot矩阵时，用set去了个重，但仅用于分析语义相似度）

语义最远的弹幕组效果没有很明显，语义确实不相同，但这样的弹幕应该有很多，距离计算的方法会导致”最远的弹幕“一般是含有不同的特征词的弹幕，而且一般该弹幕较长。

- **找到最具代表性的弹幕：**

![image-20220915143137603](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220915143137603.png)

>PS：计算所有向量重心，离重心最近的认为是最具代表性的弹幕。

**运行结果举例：**

- 前10000条：

![image-20220914182943205](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914182943205.png)

- 前20000条：

![image-20220914212327978](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914212327978.png)

- 前50000条：

![image-20220914183626868](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220914183626868.png)

**结果分析：**

前10000条最具代表性的弹幕内容是表示”来看视频了，up更新速度快“，符合B站弹幕文化。

前20000条最具代表性的弹幕内容主要是因为有个hhhhhh，B站弹幕会经常发哈哈哈哈或hhhh。

前50000条最具代表性的弹幕内容是发现该条视频是个广告视频，进行评论，符合B站发现恰饭后发弹幕的习惯。

个人觉得这个方法不是特别好，因为弹幕的话包含一整个视频时长的互动内容，内容较为分散，导致最后可能存在大量的弹幕都是0向量，所以重心会很靠近零，数据越多，内容越分散，越难得到最具代表性的弹幕。

> PS：跑这一个问如果跑全部数据的话耗时太久，故选取的是部分数据。
>

###### 8.（附加）了解一下word2vec等深度学习中常用的词向量表征（如gensim和pyltp中均有相关的库），并思考如果用这种形式的话，那么一条弹幕会被表示成什么形式？弹幕之间计算相似性的时候，会带来哪些新的问题？

**大概的理解：**

就是说我们认为，一个词的上下文的单词和它具有相似的语义，即可以理解为一个词可以被它周围的词描述。基于此，这个训练模型就是我输入一个句子的序列，然后传到隐藏层，最后再输出这个句子，不断训练这个模型。最终我们得到这个**网络的参数，就是我们语料库里所含词的向量表征**。其实目的是对输入的稀疏变量进行降维处理，与此同时相比于one-hot的表现形式，word2vec的计算给两个词语之间的相似度提供更丰富的上下文关联 ，因此应该可以更好地计算句子间的相似度。

![image-20220919103712050](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220919103712050.png)

**新的问题：**

这个弹幕语料库里的词汇，都会被表示为相同维度的向量，并且这个向量间的距离表示的是**词与词之间的语义相似度**，而非句子的语义相似度。而一条弹幕会被表示为多个向量的形式（几个向量取决于含几个词），所以如果想计算句子之间的相似度，则相当于要比较的是这些不同个数的向量。

**可能解决的办法：**

- 每一条弹幕先提取一个自己的特征词，然后比较特征词的相似度。

- 计算一条弹幕词语的平均向量，然后再比较距离。

  或者改良一下，以每个词的TF-IDF为权重，对所有词的词向量进行加权平均，获得句子的向量表征。

  （但这个方法整体感觉很不好，损失了很多信息，可能词语完全不同，最后平均向量距离很小）

- 可以利用Word Mover算法，直接度句子之间的相似度。

- gensim库里还有doc2vec，训练结果可以得到对句子的向量表征。

**代码实现：**

![image-20220919193141520](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220919193141520.png)

![image-20220919193835373](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220919193835373.png)

> 写这段才发现前面文档预处理函数写的不怎么好，因为前面没有返回过弹幕分词后的列表，也没存成文件。
>
> 为了这部分写的方便点，就在函数内做文件预处理了。
>
> PS：就是在我前面全部写完，看word2vec的时候看到一条说，一定养成把预处理的数据集写入文件的习惯，不然每次运行一次很耗时间，有点蠢了，下次一定。

**Word2Vec参数表：**

![image-20220919182852125](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220919182852125.png)

**结果展示：**

![image-20220919193743075](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20220919193743075.png)

确实很相关！都是在“粉”前后可能会出现的词！

### Ref:（打算每次把新学+参考的东西放上来）

- jieba：https://www.cnblogs.com/python-xkj/p/9247265.html 

- collections：https://docs.python.org/3/library/collections.html

- 词云图：https://zhuanlan.zhihu.com/p/242740731

- TF-IDF：https://blog.csdn.net/weixin_43734080/article/details/122226507

- one-hot：https://blog.csdn.net/Robin_Pi/article/details/103732978

- word2vec：https://www.cnblogs.com/hellojamest/p/11620176.html

  https://blog.csdn.net/liaoningxinmin/article/details/122857848

- Word Mover：https://zhuanlan.zhihu.com/p/251344868

