# 20377199 赵芮箐 第5周作业

![image-20221003080639986](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221003080639986.png)

### 任务一：实现一个基础的中文Tokenizer类

###### Tokenizer类实现：

```Python
class Tokenizer:
    def __init__(self, chars, coding='c', PDA=0):
        """
        初始化函数：初始化类，并且构建编码词典
        """
        self._chars = chars
        self._coding = coding
        self._PDA = PDA

        dic = {}
        dic['[PAD]'] = PDA
        i = 0                                   #从0开始编码
        if coding=='c':
            for char in chars:
                for ch in char:
                    if ch not in dic.keys():
                        if i == PDA: i+=1       #跳过PDA的值
                        dic[ch] = i; i+=1
        elif coding=='w':
            for char in chars:
                for ch in jieba.lcut(char):
                    if ch not in dic.keys():
                        if i == PDA: i+=1
                        dic[ch] = i; i+=1
        self._dic = dic
    
    def tokenize(self, sentence):
        """
        分词（字）函数：输入一句话，返回分词（字）后的字符列表
        """
        list_of_chars = []
        if self._coding == 'c':
            for char in sentence:
                list_of_chars.append(char)
        elif self._coding == 'w':
            list_of_chars = jieba.lcut(sentence)
        return list_of_chars
    
    def encode(self, list_of_chars):
        """
        编码函数：输入字符列表，返回转化后的数字列表
        """
        tokens = []
        for char in list_of_chars:
            tokens.append(self._dic[char])
        return tokens

    def get_seq_len(self):
        """
        得长函数:观察句子长度分布，确定一个合适的seq_len
        """
        text = self._chars
        text_len = []
        if self._coding == 'c':
            for txt in text:
                text_len.append(len(txt))
        elif self._coding  == 'w':
            for txt in text:
                text_len.append(len(jieba.lcut(txt)))
        sns.distplot(text_len)
        plt.title('text length distribution_{}'.format(self._coding))		# 标题
        plt.xlabel('length')                                        		# x轴名
        #plt.savefig('text length distribution_{}.png'.format(self._coding))
        plt.show()
        seq_len = np.percentile(text_len, 75)		#用75%分位数作为seq_len的值
        return int(seq_len)

    def trim(self, tokens, seq_len):
        """
        整长函数：输入数字列表tokens，整理数字列表的长度。不足seq_len的部分用PAD补足，超过的部分截断。
        """
        if len(tokens) >= seq_len:
            return tokens[:seq_len]
        else:
            add_len = seq_len - len(tokens)
            tokens_trim = tokens + [self._PDA]*add_len
            return tokens_trim
    
    def decode(self, tokens):
        """
        翻译函数：将模型输出的数字列表翻译回句子，如果有PAD，输出'[PAD]'
        """
        key = list(self._dic.keys())
        value = list(self._dic.values())
        chars = []
        for code in tokens:
            if code == self._PDA:
                chars.append('[PDA]')
            else:
                ch = key[value.index(code)]
                chars.append(ch)
        print("".join(chars))
    
    def encode_all(self, seq_len):
        """
        编码文本：返回所有文本(chars)的长度为seq_len的tokens
        """
        tokens_list = []
        for sen in self._chars:
            sen_token = self.trim(self.encode(self.tokenize(sen)), seq_len)
            tokens_list.append(sen_token)
        return tokens_list
```

###### 确定合适的长度（seq_len）：

- 按字划分：

  ![image-20221007175918279](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221007175918279.png)

  返回的75%分位数是44，则**取44为按字划分该文本下的seq_len**

- 按词划分：

  ![image-20221007202412152](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221007202412152.png)

  返回的75%分位数是30，则**取30为按词划分该文本下的seq_len**

###### 结果展示：

- 构建的字典

  - 按字：

  ![image-20221008192543412](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221008192543412.png)

  - 按词：

  ![image-20221008192857208](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221008192857208.png)

- 编码及解码展示

  - 按字：

  ![image-20221008192606345](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221008192606345.png)

  - 按词：

  ![image-20221008192714088](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221008192714088.png)

###### 思考：tokenizer方法与one-hot方法编码之间的区别和优劣

- 区别：

  tokenizer是通过对每个出现的字进行唯一对应的字（词）与编码的字典构建，来对句子进行编码。

  one-hot是通过对特征词的筛选，然后判断句子是否包含特征词来进行编码。

- 优劣：

  tokenizer是有序的，one-hot是无序的，所以tokenizer能确保句子的唯一性

  tokenizer的维数会比one-hot少，储存空间会比one-hot小很多。而且one-hot储存的向量，零向量很多，矩阵很稀疏，tokenizer避免浪费了这么多的空间。

### 任务二：(附加) 使用BERT预训练模型中提供的tokenizer对本次样本中的文本进行编码



### Ref：
