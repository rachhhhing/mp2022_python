# 20377199 赵芮箐 第5周作业

![image-20221003080639986](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221003080639986.png)

### 任务一：实现一个基础的中文Tokenizer类

###### 1.Tokenizer类实现

```Python
class Tokenizer:
    def __init__(self, chars, coding='c', PDA=0):
        """
        初始化函数：初始化类，并且构建编码词典
        """
        self._chars = chars
        self._coding = coding
        self._PDA = PDA

        dic = {}
        dic['[PAD]'] = PDA
        i = 0                                   #从0开始编码
        if coding=='c':
            for char in chars:
                for ch in char:
                    if ch not in dic.keys():
                        if i == PDA: i+=1       #跳过PDA的值
                        dic[ch] = i; i+=1
        elif coding=='w':
            for char in chars:
                for ch in jieba.lcut(char):
                    if ch not in dic.keys():
                        if i == PDA: i+=1
                        dic[ch] = i; i+=1
        self._dic = dic
    
    def tokenize(self, sentence):
        """
        分词（字）函数：输入一句话，返回分词（字）后的字符列表
        """
        list_of_chars = []
        if self._coding == 'c':
            for char in sentence:
                list_of_chars.append(char)
        elif self._coding == 'w':
            list_of_chars = jieba.lcut(sentence)
        return list_of_chars
    
    def encode(self, list_of_chars):
        """
        编码函数：输入字符列表，返回转化后的数字列表
        """
        tokens = []
        for char in list_of_chars:
            tokens.append(self._dic[char])
        return tokens

    def get_seq_len(self):
        """
        得长函数:观察句子长度分布，确定一个合适的seq_len
        """
        text = self._chars
        text_len = []
        if self._coding == 'c':
            for txt in text:
                text_len.append(len(txt))
        elif self._coding  == 'w':
            for txt in text:
                text_len.append(len(jieba.lcut(txt)))
        sns.distplot(text_len)
        plt.title('text length distribution_{}'.format(self._coding))	# 标题
        plt.xlabel('length')                                        	# x轴名
        #plt.savefig('text length distribution_{}.png'.format(self._coding))
        plt.show()
        seq_len = np.percentile(text_len, 75)		#用75%分位数作为seq_len的值
        return int(seq_len)

    def trim(self, tokens, seq_len):
        """
        整长函数：输入数字列表tokens，整理数字列表的长度。不足seq_len的部分用PAD补足，超过的部分截断。
        """
        if len(tokens) >= seq_len:
            return tokens[:seq_len]
        else:
            add_len = seq_len - len(tokens)
            tokens_trim = tokens + [self._PDA]*add_len
            return tokens_trim
    
    def decode(self, tokens):
        """
        翻译函数：将模型输出的数字列表翻译回句子，如果有PAD，输出'[PAD]'
        """
        key = list(self._dic.keys())
        value = list(self._dic.values())
        chars = []
        for code in tokens:
            if code == self._PDA:
                chars.append('[PDA]')
            else:
                ch = key[value.index(code)]
                chars.append(ch)
        print("".join(chars))
    
    def encode_all(self, seq_len):
        """
        编码文本：返回所有文本(chars)的长度为seq_len的tokens
        """
        tokens_list = []
        for sen in self._chars:
            sen_token = self.trim(self.encode(self.tokenize(sen)), seq_len)
            tokens_list.append(sen_token)
        return tokens_list
```

###### 2.确定合适的长度（seq_len）

- 按字划分：

  ![image-20221007175918279](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221007175918279.png)

  返回的75%分位数是44，则**取44为按字划分该文本下的seq_len**

- 按词划分：

  ![image-20221007202412152](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221007202412152.png)

  返回的75%分位数是30，则**取30为按词划分该文本下的seq_len**

###### 3.结果展示

- 构建的字典（部分）

  - 按字：

  ![image-20221008192543412](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221008192543412.png)

  - 按词：

  ![image-20221008192857208](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221008192857208.png)

- 编码及解码

  - 按字：

  ![image-20221008192606345](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221008192606345.png)

  - 按词：

  ![image-20221008192714088](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221008192714088.png)

###### 4.思考：tokenizer方法与one-hot方法编码之间的区别和优劣

- 区别：

  tokenizer是通过对每个出现的字进行唯一对应的字（词）与编码的字典构建，来对句子进行编码。

  one-hot是通过对特征词的筛选，然后判断句子是否包含特征词来进行编码。

- 优劣：

  tokenizer是有序的，one-hot是无序的，所以tokenizer能确保句子的唯一性

  tokenizer的维数会比one-hot少，储存空间会比one-hot小很多。而且one-hot储存的向量，零向量很多，矩阵很稀疏，tokenizer避免浪费了这么多的空间。

### 任务二：(附加) 使用BERT预训练模型中提供的tokenizer

###### 1.对本次样本中的文本进行编码

```python
from transformers import BertTokenizer

bert_name = 'bert-base-chinese'
tokenizer = BertTokenizer.from_pretrained(bert_name)  

input_ids = tokenizer.encode(
                        text = data[1],
                        text_pair = data[4],      # 可以编码一个/两个句子
                        truncation=True,          # 当句子长度大于max_length时截断
                        padding='max_length',     # 补pad到max_length长度
                        add_special_tokens=True,  # 添加special tokens,也就是CLS和SEP
                        max_length=30,            # 设定最大文本长度
                        return_tensors=None       # 可取值tf,pt,np,默认为返回list
					)
```

![image-20221010141541012](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221010141541012.png)

这里用的就是最简单的编码函数，还有增强的编码函数、批量编码句子函数和批量编码成对句子函数！很好用了就是说。还可以看到的是这个tokenizer划分句子是按字划分的。

> BERT模型的tokenizer除了[PAD]这个特殊编码外，还有另外俩特殊编码。
>
> [CLS]：将对应的输出作为文本的语义表示，会用于文本分类任务
>
> [SEP]：对输入的两句话用一个[SEP]符号作分割，并分别对两句话附加两个不同的文本向量以作区分。

###### 2.使用其预训练模型得到句子的表征向量

```python
from transformers import BertModel

bert_name = 'bert-base-chinese'
bert_model = BertModel.from_pretrained(bert_name)

tokens = tokenizer.encode_plus(text=data[1], return_tensors='pt')
sen_vec = bert_model(**tokens)
```

![image-20221010163417772](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221010163417772.png)

> 这里输出的是是模型最后一层输出的隐藏状态，实际上是每个词会被转换成**768维**的向量表示，然后再共同组成句子的表征向量

###### 3.抽取本次数据中的一条文本，并查找其相似文本

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

sbert_model = SentenceTransformer(bert_name)

sentence_embeddings = sbert_model.encode(data)
#计算第一句向量和其余句子的余弦相似度，即语义相似度
comp_sim = cosine_similarity([sentence_embeddings[1]],sentence_embeddings[1:])
```

![image-20221010182315642](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20221010182315642.png)

> PS：就选了前100个句子进行分析

###### 4.思考：与第二周中使用word2wec得到的结果相比较，分析两者优劣

- 无论是Bert还是word2vec，对词语的表征都利用了周围词的信息。但Bert比word2vec更好在还考虑了词序
- 对于语义相反词的问题，Bert处理的会更好，因为其预训练中，同时训练了next sentence prediction，相比word2vec而言，就会避免句子几乎相似只存在一个反义词的问题。

### 代码：

https://github.com/rachhhhing/mp2022_python/blob/master/week5/week5.py

### Ref：

- BERT：https://zhuanlan.zhihu.com/p/98855346

- huggingface使用

  视频教程：https://www.bilibili.com/video/BV1a44y1H7Jc

  对应代码：https://github.com/lansinuote/Huggingface_Toturials

- BERT生成句向量+文本相似度：https://zhuanlan.zhihu.com/p/375495030
